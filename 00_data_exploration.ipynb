{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00e4a19e-d9b8-4ff1-b9c1-d1041334dc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import scipy.io\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py # Import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccf9a769-9c24-4c72-a0d5-93174b541012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths (adjust these to your actual paths)\n",
    "TVPARM_DATA_DIR = r\"C:\\Users\\Omen\\OneDrive\\Documents\\AI\\AgenticAIWorkspace\\video_summarization_project\\data\\tvsum\"\n",
    "VIDEO_DIR = os.path.join(TVPARM_DATA_DIR, 'ydata-tvsum50-video') # This directory will contain individual MP4 videos\n",
    "ANNOTATION_DIR = os.path.join(TVPARM_DATA_DIR, 'ydata-tvsum50-matlab') # This directory contains ydata-tvsum50.mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db6349f6-74e6-4687-8eb8-a3bf2dc9a58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video directory exists: True\n",
      "Annotation directory exists: True\n"
     ]
    }
   ],
   "source": [
    "# 1. Check if directories exist\n",
    "print(f\"Video directory exists: {os.path.exists(VIDEO_DIR)}\")\n",
    "print(f\"Annotation directory exists: {os.path.exists(ANNOTATION_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "178fc136-63a2-4385-a35a-fd3620e886c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 50 video files. First 5:\n",
      "- -esJrBWj2d8.mp4\n",
      "- 0tmA_C6XwfM.mp4\n",
      "- 37rzWOQsNIw.mp4\n",
      "- 3eYKfiOEJNs.mp4\n",
      "- 4wU_LUjG5Ic.mp4\n"
     ]
    }
   ],
   "source": [
    "# 2. List some video files (Assuming individual video files are in ydata-tvsum50-video)\n",
    "video_files = glob.glob(os.path.join(VIDEO_DIR, '*.mp4'))\n",
    "print(f\"\\nFound {len(video_files)} video files. First 5:\")\n",
    "for i, vf in enumerate(video_files[:5]):\n",
    "    print(f\"- {os.path.basename(vf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d234214-4fe6-4d4d-9053-4dbe886bd3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 1 annotation files. First 5:\n",
      "- ydata-tvsum50.mat\n"
     ]
    }
   ],
   "source": [
    "# 3. List some annotation files\n",
    "annotation_files = glob.glob(os.path.join(ANNOTATION_DIR, '*.mat'))\n",
    "print(f\"\\nFound {len(annotation_files)} annotation files. First 5:\")\n",
    "for i, af in enumerate(annotation_files[:5]):\n",
    "    print(f\"- {os.path.basename(af)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d6ae738-9087-4149-af1f-59327618b928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video directory exists: True\n",
      "Annotation directory exists: True\n",
      "\n",
      "Found 0 video files. First 5:\n"
     ]
    }
   ],
   "source": [
    "# 00_data_exploration.ipynb\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import scipy.io\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py # Import h5py\n",
    "\n",
    "TVPARM_BASE_DIR = r'C:\\Users\\Omen\\OneDrive\\Documents\\AI\\AgenticAIWorkspace\\video_summarization_project\\ydata-tvsum50-v1_1'\n",
    "\n",
    "# Now, define the paths to the video and annotation directories relative to the base\n",
    "VIDEO_DIR = os.path.join(TVPARM_BASE_DIR, 'ydata-tvsum50-video')\n",
    "ANNOTATION_DIR = os.path.join(TVPARM_BASE_DIR, 'ydata-tvsum50-matlab')\n",
    "\n",
    "# 1. Check if directories exist\n",
    "print(f\"Video directory exists: {os.path.exists(VIDEO_DIR)}\")\n",
    "print(f\"Annotation directory exists: {os.path.exists(ANNOTATION_DIR)}\")\n",
    "\n",
    "# 2. List some video files (Assuming individual video files are in ydata-tvsum50-video)\n",
    "video_files = glob.glob(os.path.join(VIDEO_DIR, '*.mp4'))\n",
    "print(f\"\\nFound {len(video_files)} video files. First 5:\")\n",
    "for i, vf in enumerate(video_files[:5]):\n",
    "    print(f\"- {os.path.basename(vf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2ee46b8-47eb-4a43-9e81-f16da5ed11ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Failed to load all required annotation data (gt_score, nframes, change_points).\n"
     ]
    }
   ],
   "source": [
    "# 3. Specifically target the ydata-tvsum50.mat file\n",
    "main_annotation_file = os.path.join(ANNOTATION_DIR, 'ydata-tvsum50.mat')\n",
    "\n",
    "# We won't try to load a single tvsum_data_np here, as the structure is different.\n",
    "# Instead, we'll load individual components.\n",
    "all_gt_scores = None # Initialize to None\n",
    "all_n_frames = None\n",
    "all_change_points = None\n",
    "all_video_titles = None\n",
    "all_user_scores = None # New\n",
    "\n",
    "if os.path.exists(main_annotation_file):\n",
    "    print(f\"\\nFound main annotation file: {os.path.basename(main_annotation_file)}\")\n",
    "    print(f\"\\nInspecting main annotation: {os.path.basename(main_annotation_file)}\")\n",
    "\n",
    "    # Use h5py to open the .mat file\n",
    "    with h5py.File(main_annotation_file, 'r') as f:\n",
    "        print(\"Keys in .mat file (using h5py):\")\n",
    "        for key in f.keys():\n",
    "            print(f\"- {key}\")\n",
    "\n",
    "        if 'tvsum50' in f:\n",
    "            tvsum_group = f['tvsum50'] # Access the 'tvsum50' Group\n",
    "            print(f\"\\nType of f['tvsum50']: {type(tvsum_group)}\")\n",
    "\n",
    "            print(f\"Keys inside the 'tvsum50' group:\")\n",
    "            for key in tvsum_group.keys():\n",
    "                print(f\"- {key}\")\n",
    "\n",
    "            # Now, load each relevant dataset directly from the group\n",
    "            try:\n",
    "                # Load ground truth scores (this should be a 2D array: (num_videos, max_segments))\n",
    "                # For each video, the gt_score will be an array of importance scores for its segments.\n",
    "                all_gt_scores = tvsum_group['gt_score'][()] # Use [()] to load the whole dataset\n",
    "                print(f\"Loaded 'gt_score'. Shape: {all_gt_scores.shape}\")\n",
    "\n",
    "                # Load number of frames for each video\n",
    "                all_n_frames = tvsum_group['nframes'][()]\n",
    "                print(f\"Loaded 'nframes'. Shape: {all_n_frames.shape}\")\n",
    "\n",
    "                # Load change points for each video (these define segments)\n",
    "                all_change_points = tvsum_group['change_points'][()]\n",
    "                print(f\"Loaded 'change_points'. Shape: {all_change_points.shape}\")\n",
    "\n",
    "                # Load video titles (often stored as references to strings)\n",
    "                # You'll need to dereference these if they are HDF5 references\n",
    "                video_title_refs = tvsum_group['title'][()]\n",
    "                all_video_titles = []\n",
    "                for ref in video_title_refs.flatten(): # Flatten to iterate through refs if they are nested\n",
    "                     # Sometimes titles are directly stored, sometimes as refs\n",
    "                    if isinstance(ref, h5py.Reference):\n",
    "                        title_bytes = f[ref][()].flatten()[0] # Access the string from the reference\n",
    "                        all_video_titles.append(title_bytes.decode('utf-16le')) # Decode string, might be utf-16\n",
    "                    else:\n",
    "                        # If not a reference, assume it's a direct string or bytes (e.g., from an older .mat file)\n",
    "                        all_video_titles.append(ref.decode('utf-16le')) # Try decoding if bytes\n",
    "                print(f\"Loaded 'title'. First 5: {all_video_titles[:5]}\")\n",
    "\n",
    "                # Load user annotations (for more detailed analysis, bonus)\n",
    "                # This is typically a list of references to arrays of user scores.\n",
    "                user_anno_refs = tvsum_group['user_anno'][()]\n",
    "                all_user_scores = []\n",
    "                for ref in user_anno_refs.flatten():\n",
    "                    if isinstance(ref, h5py.Reference):\n",
    "                        all_user_scores.append(f[ref][()])\n",
    "                    # else: this case might not happen for user_anno\n",
    "                print(f\"Loaded 'user_anno'. Length: {len(all_user_scores)}\")\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading datasets from 'tvsum50' group: {e}\")\n",
    "                print(\"Could not load annotation data from expected structure.\")\n",
    "        else:\n",
    "            print(\"Error: 'tvsum50' key not found in the .mat file. Check file integrity or source.\")\n",
    "\n",
    "# Now, we process the loaded data if successful\n",
    "if all_gt_scores is not None and all_n_frames is not None and all_change_points is not None:\n",
    "    print(\"\\nAnnotation data loaded successfully. Proceeding with exploration.\")\n",
    "\n",
    "    # Let's inspect data for the first video (index 0)\n",
    "    video_idx = 0\n",
    "    if video_idx < len(all_gt_scores):\n",
    "        print(f\"\\n--- Data for video index {video_idx} ---\")\n",
    "\n",
    "        # Get the GT score for the first video.\n",
    "        # It's an array of arrays/lists of importance scores for segments.\n",
    "        # We need to access the specific segment importance for this video.\n",
    "        # This typically means f[all_gt_scores[video_idx, 0]] if it's a reference.\n",
    "        # Or if it's already an array of arrays, then all_gt_scores[video_idx]\n",
    "        # Let's assume it's a reference that needs to be dereferenced.\n",
    "        current_video_gt_score_ref = all_gt_scores[video_idx, 0] # Often a (N,1) array of refs\n",
    "        gt_score = f[current_video_gt_score_ref][:] # Dereference the HDF5 object link and load content\n",
    "\n",
    "        print(f\"gt_score shape for video {video_idx}: {gt_score.shape}\")\n",
    "        print(f\"gt_score (first 10 values) for video {video_idx}:\\n{gt_score.flatten()[:10]}\")\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(gt_score.flatten())\n",
    "        plt.title(f\"Ground Truth Importance Score for Video {video_idx} (Title: {all_video_titles[video_idx] if all_video_titles else 'N/A'})\")\n",
    "        plt.xlabel(\"Segment Index\")\n",
    "        plt.ylabel(\"Importance Score\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        n_frames = all_n_frames[video_idx, 0] # Often stored as a 2D array (N,1)\n",
    "        print(f\"Total frames in video {video_idx}: {int(n_frames)}\")\n",
    "\n",
    "        change_points_ref = all_change_points[video_idx, 0]\n",
    "        change_points = f[change_points_ref][:]\n",
    "        print(f\"Change points shape for video {video_idx}: {change_points.shape}\")\n",
    "        print(f\"First 10 change points for video {video_idx}:\\n{change_points.flatten()[:10]}\")\n",
    "        print(f\"Number of segments (change points): {len(change_points)}\") # Or len(gt_score)\n",
    "\n",
    "        if all_user_scores and video_idx < len(all_user_scores):\n",
    "            user_score = all_user_scores[video_idx]\n",
    "            print(f\"user_score shape for video {video_idx}: {user_score.shape}\")\n",
    "            print(f\"Number of users who annotated this video: {user_score.shape[0]}\")\n",
    "            print(f\"First user's scores (first 10 values):\\n{user_score[0, :10].flatten()}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Video index {video_idx} is out of bounds for the loaded data.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nFailed to load all required annotation data (gt_score, nframes, change_points).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "900eac65-ff44-4b0e-964a-dec1ee2188ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0   1                                                  2\n",
      "0  AwmHb44_ouw  VT  4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,...\n",
      "1  AwmHb44_ouw  VT  2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,...\n",
      "2  AwmHb44_ouw  VT  3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,...\n",
      "3  AwmHb44_ouw  VT  4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,...\n",
      "4  AwmHb44_ouw  VT  2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the file is in 'data/tvsum/'\n",
    "anno_df = pd.read_csv(r'C:\\Users\\Omen\\OneDrive\\Documents\\AI\\AgenticAIWorkspace\\video_summarization_project\\data\\tvsum\\ydata-tvsum50-anno.tsv', sep='\\t', header=None)\n",
    "print(anno_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ebf2da8-d1b0-4cb0-ac87-64d098f2d397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the absolute base directory where you extracted 'ydata-tvsum50-v1_1'\n",
    "# Please ensure this path matches your system exactly\n",
    "TVPARM_BASE_DIR = r'C:\\Users\\Omen\\OneDrive\\Documents\\AI\\AgenticAIWorkspace\\video_summarization_project\\ydata-tvsum50-v1_1'\n",
    "\n",
    "# Now, define the paths to the video and annotation directories relative to the base\n",
    "VIDEO_DIR = os.path.join(TVPARM_BASE_DIR, 'ydata-tvsum50-video')\n",
    "ANNOTATION_DIR = os.path.join(TVPARM_BASE_DIR, 'ydata-tvsum50-matlab')\n",
    "\n",
    "# The main annotation file path will then correctly point to the .mat file\n",
    "# main_annotation_file = os.path.join(ANNOTATION_DIR, 'ydata-tvsum50.mat') # This line can remain as is if you have it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d611960-8ceb-4e1a-b152-85985816005b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Omen\\anaconda3\\envs\\inenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Omen\\anaconda3\\envs\\inenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting feature extraction. Videos from: C:\\Users\\Omen\\OneDrive\\Documents\\AI\\AgenticAIWorkspace\\video_summarization_project\\ydata-tvsum50-v1_1\\ydata-tvsum50-video, saving to: data/extracted_features\n",
      "Found 50 videos to process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|█████████████████████████████████████████████████████████████| 50/50 [14:16<00:00, 17.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm # For progress bars\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set the base directory for your TVSum dataset\n",
    "TVPARM_BASE_DIR = r'C:\\Users\\Omen\\OneDrive\\Documents\\AI\\AgenticAIWorkspace\\video_summarization_project\\ydata-tvsum50-v1_1'\n",
    "VIDEO_DIR = os.path.join(TVPARM_BASE_DIR, 'ydata-tvsum50-video')\n",
    "FEATURES_SAVE_DIR = 'data/extracted_features' # Where to save processed features\n",
    "\n",
    "# Ensure the save directory exists\n",
    "os.makedirs(FEATURES_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Choose your CNN model (ResNet-50 is a good balance)\n",
    "# pretrained=True means it uses weights trained on a large image dataset (ImageNet)\n",
    "model = models.resnet50(pretrained=True)\n",
    "# We want features, not classifications, so remove the last classification layer\n",
    "model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "model.eval() # Set model to evaluation mode (no learning updates, turn off dropout etc.)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define image transformations needed for the CNN\n",
    "# All pre-trained models expect 224x224 images and specific normalization\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(), # Convert OpenCV BGR image to PIL image\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(), # Convert to PyTorch Tensor, scales to [0,1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # ImageNet normalization\n",
    "])\n",
    "\n",
    "def extract_features_from_video(video_path, frame_sampling_rate=15):\n",
    "    \"\"\"\n",
    "    Extracts features from a video using a pre-trained CNN.\n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        frame_sampling_rate (int): Process every 'n' frames to reduce data size.\n",
    "                                   e.g., 15 means process every 15th frame.\n",
    "    Returns:\n",
    "        np.ndarray: Array of features, shape (num_sampled_frames, feature_dim).\n",
    "                    Returns None if video cannot be opened.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        return None\n",
    "\n",
    "    features_list = []\n",
    "    frame_count = 0\n",
    "\n",
    "    with torch.no_grad(): # Don't compute gradients; just forward pass\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break # No more frames\n",
    "\n",
    "            if frame_count % frame_sampling_rate == 0:\n",
    "                # Convert BGR (OpenCV default) to RGB (PyTorch/PIL default)\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Preprocess the frame and add a batch dimension\n",
    "                input_tensor = preprocess(frame_rgb).unsqueeze(0) # (1, C, H, W)\n",
    "                input_tensor = input_tensor.to(device)\n",
    "\n",
    "                # Get features\n",
    "                with torch.no_grad():\n",
    "                    feature = model(input_tensor) # (1, feature_dim, 1, 1) if avg_pool is last\n",
    "                \n",
    "                # Squeeze the 1x1 spatial dimensions and move to CPU\n",
    "                features_list.append(feature.squeeze().cpu().numpy())\n",
    "\n",
    "            frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    if features_list:\n",
    "        return np.array(features_list)\n",
    "    else:\n",
    "        return np.array([]) # Return empty array if no frames processed\n",
    "\n",
    "def process_all_tvsum_videos(video_dir, save_dir):\n",
    "    \"\"\"\n",
    "    Processes all videos in the TVSum video directory and saves their features.\n",
    "    \"\"\"\n",
    "    video_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]\n",
    "    \n",
    "    print(f\"Found {len(video_files)} videos to process...\")\n",
    "    for video_name in tqdm(video_files, desc=\"Extracting Features\"):\n",
    "        video_path = os.path.join(video_dir, video_name)\n",
    "        save_path = os.path.join(save_dir, video_name.replace('.mp4', '_features.npy'))\n",
    "\n",
    "        if os.path.exists(save_path):\n",
    "            # print(f\"Features for {video_name} already exist. Skipping.\")\n",
    "            continue # Skip if already processed\n",
    "\n",
    "        features = extract_features_from_video(video_path, frame_sampling_rate=15)\n",
    "        if features is not None and features.size > 0:\n",
    "            np.save(save_path, features)\n",
    "            # print(f\"Saved features for {video_name} with shape {features.shape}\")\n",
    "        else:\n",
    "            print(f\"Warning: No features extracted for {video_name}.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(f\"Starting feature extraction. Videos from: {VIDEO_DIR}, saving to: {FEATURES_SAVE_DIR}\")\n",
    "    process_all_tvsum_videos(VIDEO_DIR, FEATURES_SAVE_DIR)\n",
    "    print(\"Feature extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d102fa3-fd32-4d05-9c96-e8914b7566c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing TVSumDataset (with approximate segment mapping) ---\n",
      "Preparing dataset (loading features and aligning GT with approximation)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Annotations: 100%|█████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 32.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished dataset preparation. Total videos loaded: 50\n",
      "\n",
      "First video features shape: torch.Size([707, 2048])\n",
      "First video importance scores shape: torch.Size([707])\n",
      "First video name: AwmHb44_ouw\n",
      "\n",
      "Iterating through a batch from DataLoader:\n",
      "Batch 1:\n",
      "  Features shape (padded): torch.Size([2, 417, 2048])\n",
      "  Scores shape (padded): torch.Size([2, 417])\n",
      "  Original lengths: tensor([417, 376])\n",
      "  Video Names: ['LRw_obCPUt0', 'XkqCExn6_Us']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# --- Configuration (match feature_extraction.py) ---\n",
    "TVPARM_BASE_DIR = r'C:\\Users\\Omen\\OneDrive\\Documents\\AI\\AgenticAIWorkspace\\video_summarization_project\\ydata-tvsum50-v1_1'\n",
    "MATLAB_ANNOTATION_FILE = os.path.join(TVPARM_BASE_DIR, 'ydata-tvsum50-matlab', 'ydata-tvsum50.mat')\n",
    "FEATURES_SAVE_DIR = 'data/extracted_features' # Where you saved processed features\n",
    "\n",
    "class TVSumDataset(Dataset):\n",
    "    def __init__(self, features_dir, annotation_file, sample_rate=15):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by loading features and annotations.\n",
    "        Adapts to missing 'change_points' by approximating segment importance.\n",
    "        Args:\n",
    "            features_dir (str): Directory containing saved .npy feature files.\n",
    "            annotation_file (str): Path to ydata-tvsum50.mat.\n",
    "            sample_rate (int): Frame sampling rate used during feature extraction.\n",
    "                               Used for conceptual alignment, but actual alignment\n",
    "                               is now based on proportional mapping of gt_score.\n",
    "        \"\"\"\n",
    "        self.features_dir = features_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.data = [] # List to store (features, importance_scores, video_name) tuples\n",
    "\n",
    "        self.video_annotations = self._load_annotations(annotation_file)\n",
    "        \n",
    "        feature_files = [f for f in os.listdir(features_dir) if f.endswith('_features.npy')]\n",
    "        # Create a set of YouTube IDs for which features exist, for quick lookup\n",
    "        video_ids_with_features = {f.replace('_features.npy', '') for f in feature_files}\n",
    "\n",
    "        print(\"Preparing dataset (loading features and aligning GT with approximation)...\")\n",
    "        # Iterate through annotations using the reliable YouTube ID as key\n",
    "        for youtube_id, anno in tqdm(self.video_annotations.items(), desc=\"Loading Annotations\"):\n",
    "            \n",
    "            # Check if features for this YouTube ID actually exist\n",
    "            if youtube_id not in video_ids_with_features:\n",
    "                # print(f\"Features not found for video: {youtube_id}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            feature_filename = f\"{youtube_id}_features.npy\"\n",
    "            feature_path = os.path.join(features_dir, feature_filename)\n",
    "\n",
    "            try:\n",
    "                features = np.load(feature_path)\n",
    "                \n",
    "                gt_score = anno['gt_score'].flatten()\n",
    "                \n",
    "                num_sampled_frames = features.shape[0]\n",
    "                num_original_segments = len(gt_score)\n",
    "                \n",
    "                frame_importance = np.zeros(num_sampled_frames, dtype=np.float32)\n",
    "\n",
    "                if num_original_segments > 0:\n",
    "                    sampled_frames_per_original_segment = num_sampled_frames / num_original_segments\n",
    "\n",
    "                    for i in range(num_original_segments):\n",
    "                        start_idx = int(round(i * sampled_frames_per_original_segment))\n",
    "                        end_idx = int(round((i + 1) * sampled_frames_per_original_segment))\n",
    "                        \n",
    "                        start_idx = max(0, min(start_idx, num_sampled_frames))\n",
    "                        end_idx = max(0, min(end_idx, num_sampled_frames))\n",
    "\n",
    "                        if start_idx < end_idx:\n",
    "                            frame_importance[start_idx:end_idx] = gt_score[i]\n",
    "                        elif start_idx < num_sampled_frames:\n",
    "                             frame_importance[start_idx] = gt_score[i]\n",
    "                \n",
    "                self.data.append({\n",
    "                    'features': torch.tensor(features, dtype=torch.float32),\n",
    "                    'importance_scores': torch.tensor(frame_importance, dtype=torch.float32),\n",
    "                    'video_name': youtube_id\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing video {youtube_id}: {e}. Skipping.\")\n",
    "\n",
    "        print(f\"Finished dataset preparation. Total videos loaded: {len(self.data)}\")\n",
    "\n",
    "    def _load_annotations(self, annotation_file):\n",
    "        \"\"\"\n",
    "        Helper to load annotations from the .mat file.\n",
    "        This version now loads YouTube IDs directly from the 'video' field.\n",
    "        Returns a dict mapping video YouTube ID to its annotation data.\n",
    "        \"\"\"\n",
    "        annotations = {}\n",
    "        with h5py.File(annotation_file, 'r') as f:\n",
    "            if 'tvsum50' in f:\n",
    "                tvsum_group = f['tvsum50']\n",
    "                \n",
    "                video_title_refs = tvsum_group['title'][()]\n",
    "                user_anno_refs = tvsum_group['user_anno'][()]\n",
    "                gt_score_refs = tvsum_group['gt_score'][()]\n",
    "                # --- NEW: Get reference to the 'video' (YouTube ID) dataset ---\n",
    "                video_id_refs = tvsum_group['video'][()]\n",
    "                # --- END NEW ---\n",
    "\n",
    "                for i in range(len(video_title_refs)):\n",
    "                    # Get video title (from previous fix)\n",
    "                    title_char_array = f[video_title_refs[i, 0]][()]\n",
    "                    video_title = \"\".join(chr(c) for c in title_char_array.flatten() if c != 0)\n",
    "                    \n",
    "                    # --- NEW: Get YouTube ID directly from 'video' field ---\n",
    "                    youtube_id_char_array = f[video_id_refs[i, 0]][()]\n",
    "                    youtube_id = \"\".join(chr(c) for c in youtube_id_char_array.flatten() if c != 0)\n",
    "                    \n",
    "                    if not youtube_id:\n",
    "                        print(f\"Warning: Skipping annotation for '{video_title}' - extracted YouTube ID is empty. Raw ID data: {youtube_id_char_array}\")\n",
    "                        continue\n",
    "                    # --- END NEW ---\n",
    "\n",
    "                    gt_score = f[gt_score_refs[i, 0]][()]\n",
    "                    user_anno = f[user_anno_refs[i, 0]][()]\n",
    "                    \n",
    "                    annotations[youtube_id] = {\n",
    "                        'title': video_title,\n",
    "                        'gt_score': gt_score,\n",
    "                        'user_anno': user_anno,\n",
    "                    }\n",
    "            else:\n",
    "                raise RuntimeError(\"Key 'tvsum50' not found in the .mat file. Check file integrity.\")\n",
    "        return annotations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# This collate_fn remains the same\n",
    "def collate_fn(batch):\n",
    "    max_len = max([item['features'].shape[0] for item in batch])\n",
    "    \n",
    "    padded_features = torch.zeros(len(batch), max_len, batch[0]['features'].shape[1])\n",
    "    padded_importance_scores = torch.zeros(len(batch), max_len)\n",
    "    \n",
    "    lengths = []\n",
    "    video_names = []\n",
    "\n",
    "    for i, item in enumerate(batch):\n",
    "        seq_len = item['features'].shape[0]\n",
    "        padded_features[i, :seq_len, :] = item['features']\n",
    "        padded_importance_scores[i, :seq_len] = item['importance_scores']\n",
    "        lengths.append(seq_len)\n",
    "        video_names.append(item['video_name'])\n",
    "    \n",
    "    lengths, perm_idx = torch.tensor(lengths).sort(descending=True)\n",
    "    padded_features = padded_features[perm_idx]\n",
    "    padded_importance_scores = padded_importance_scores[perm_idx]\n",
    "    video_names = [video_names[i] for i in perm_idx]\n",
    "\n",
    "    return padded_features, padded_importance_scores, lengths, video_names\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- Testing TVSumDataset (with approximate segment mapping) ---\")\n",
    "    tvsum_dataset = TVSumDataset(FEATURES_SAVE_DIR, MATLAB_ANNOTATION_FILE)\n",
    "\n",
    "    if len(tvsum_dataset) > 0:\n",
    "        first_video_data = tvsum_dataset[0]\n",
    "        print(f\"\\nFirst video features shape: {first_video_data['features'].shape}\")\n",
    "        print(f\"First video importance scores shape: {first_video_data['importance_scores'].shape}\")\n",
    "        print(f\"First video name: {first_video_data['video_name']}\")\n",
    "\n",
    "        train_dataloader = DataLoader(tvsum_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "        print(\"\\nIterating through a batch from DataLoader:\")\n",
    "        for i, (features, scores, lengths, names) in enumerate(train_dataloader):\n",
    "            print(f\"Batch {i+1}:\")\n",
    "            print(f\"  Features shape (padded): {features.shape}\")\n",
    "            print(f\"  Scores shape (padded): {scores.shape}\")\n",
    "            print(f\"  Original lengths: {lengths}\")\n",
    "            print(f\"  Video Names: {names}\")\n",
    "            if i == 0: break\n",
    "    else:\n",
    "        print(\"Dataset is empty. Please ensure feature_extraction.py ran successfully and paths are correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5ba61f0-6f70-4261-b2d7-95fe9a5d376e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting structure of: C:\\Users\\Omen\\OneDrive\\Documents\\AI\\AgenticAIWorkspace\\video_summarization_project\\ydata-tvsum50-v1_1\\ydata-tvsum50-matlab\\ydata-tvsum50.mat\n",
      "\n",
      "- tvsum50 (Group)\n",
      "  - category (Dataset)\n",
      "    Shape: (50, 1), Dtype: object\n",
      "  - gt_score (Dataset)\n",
      "    Shape: (50, 1), Dtype: object\n",
      "  - length (Dataset)\n",
      "    Shape: (50, 1), Dtype: object\n",
      "  - nframes (Dataset)\n",
      "    Shape: (50, 1), Dtype: object\n",
      "  - title (Dataset)\n",
      "    Shape: (50, 1), Dtype: object\n",
      "  - user_anno (Dataset)\n",
      "    Shape: (50, 1), Dtype: object\n",
      "  - video (Dataset)\n",
      "    Shape: (50, 1), Dtype: object\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "\n",
    "# --- Configuration (Use your exact path to ydata-tvsum50.mat) ---\n",
    "TVPARM_BASE_DIR = r'C:\\Users\\Omen\\OneDrive\\Documents\\AI\\AgenticAIWorkspace\\video_summarization_project\\ydata-tvsum50-v1_1'\n",
    "MATLAB_ANNOTATION_FILE = os.path.join(TVPARM_BASE_DIR, 'ydata-tvsum50-matlab', 'ydata-tvsum50.mat')\n",
    "\n",
    "def print_hdf5_structure(name, obj, indent=''):\n",
    "    \"\"\"Recursively prints the structure of an HDF5 group or dataset.\"\"\"\n",
    "    print(f\"{indent}- {name} ({'Group' if isinstance(obj, h5py.Group) else 'Dataset'})\")\n",
    "    if isinstance(obj, h5py.Dataset):\n",
    "        try:\n",
    "            # For datasets, print shape and dtype\n",
    "            print(f\"{indent}  Shape: {obj.shape}, Dtype: {obj.dtype}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{indent}  (Could not get shape/dtype: {e})\")\n",
    "    elif isinstance(obj, h5py.Group):\n",
    "        # Recursively call for members of a group\n",
    "        for key in obj.keys():\n",
    "            print_hdf5_structure(key, obj[key], indent + '  ')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(MATLAB_ANNOTATION_FILE):\n",
    "        print(f\"Error: .mat file not found at {MATLAB_ANNOTATION_FILE}\")\n",
    "    else:\n",
    "        print(f\"Inspecting structure of: {MATLAB_ANNOTATION_FILE}\\n\")\n",
    "        try:\n",
    "            with h5py.File(MATLAB_ANNOTATION_FILE, 'r') as f:\n",
    "                if 'tvsum50' in f:\n",
    "                    tvsum_group = f['tvsum50']\n",
    "                    print_hdf5_structure('tvsum50', tvsum_group)\n",
    "                else:\n",
    "                    print(\"Top-level key 'tvsum50' not found.\")\n",
    "                \n",
    "                # Also, search for 'change_points' specifically in the entire file\n",
    "                found_change_points = False\n",
    "                for item_name in f.keys():\n",
    "                    f.visititems(lambda name, obj: print(f\"Found 'change_points' at: {name}\") if 'change_points' in name else None)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while inspecting the HDF5 file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9abea781-5049-47b3-9971-3f8ef7f7a494",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \u001b[38;5;66;03m# Import os for saving models\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Import your defined Dataset and Model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Ensure FEATURES_SAVE_DIR and MATLAB_ANNOTATION_FILE are correctly set in dataset.py\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TVSumDataset, FEATURES_SAVE_DIR, MATLAB_ANNOTATION_FILE, collate_fn\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VideoSummarizerLSTM\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# --- Hyperparameters (You can tune these!) ---\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dataset'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import os # Import os for saving models\n",
    "\n",
    "# Import your defined Dataset and Model\n",
    "# Ensure FEATURES_SAVE_DIR and MATLAB_ANNOTATION_FILE are correctly set in dataset.py\n",
    "from dataset import TVSumDataset, FEATURES_SAVE_DIR, MATLAB_ANNOTATION_FILE, collate_fn\n",
    "from model import VideoSummarizerLSTM\n",
    "\n",
    "# --- Hyperparameters (You can tune these!) ---\n",
    "FEATURE_DIM = 2048       # ResNet-50 output feature dimension\n",
    "HIDDEN_DIM = 512         # LSTM hidden state size\n",
    "NUM_LSTM_LAYERS = 2      # Number of LSTM layers\n",
    "DROPOUT_RATE = 0.5       # Dropout for regularization\n",
    "BIDIRECTIONAL = True     # Use Bidirectional LSTM (often performs better)\n",
    "BATCH_SIZE = 4           # How many videos to process at once (adjust based on GPU memory)\n",
    "LEARNING_RATE = 0.0001   # Learning rate for the optimizer\n",
    "NUM_EPOCHS = 50          # How many times to loop through the entire dataset\n",
    "SPLIT_RATIO = 0.8        # 80% for training, 20% for validation/testing\n",
    "\n",
    "# --- Device Configuration ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Model Saving Configuration ---\n",
    "MODEL_SAVE_DIR = 'checkpoints'\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True) # Ensure the directory exists\n",
    "\n",
    "def train_model():\n",
    "    # 1. Load Dataset\n",
    "    print(\"Loading TVSum Dataset...\")\n",
    "    full_dataset = TVSumDataset(FEATURES_SAVE_DIR, MATLAB_ANNOTATION_FILE)\n",
    "    \n",
    "    # Check if dataset is empty - crucial check!\n",
    "    if len(full_dataset) == 0:\n",
    "        print(\"Error: Dataset is empty. Cannot start training.\")\n",
    "        print(\"Please ensure feature_extraction.py ran successfully and paths in dataset.py are correct.\")\n",
    "        return\n",
    "\n",
    "    # Split into training and validation sets\n",
    "    train_size = int(SPLIT_RATIO * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    print(f\"Training on {len(train_dataset)} videos, Validating on {len(val_dataset)} videos.\")\n",
    "\n",
    "    # 2. Initialize Model, Loss Function, and Optimizer\n",
    "    model = VideoSummarizerLSTM(FEATURE_DIM, HIDDEN_DIM, num_layers=NUM_LSTM_LAYERS, \n",
    "                                dropout=DROPOUT_RATE, bidirectional=BIDIRECTIONAL).to(device)\n",
    "    \n",
    "    # Loss function: Mean Squared Error is common for importance score prediction\n",
    "    criterion = nn.MSELoss() \n",
    "    \n",
    "    # Optimizer: Adam is a good general-purpose choice\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # 3. Training Loop\n",
    "    print(\"\\nStarting training...\")\n",
    "    best_val_loss = float('inf') # To save the best model\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train() # Set model to training mode\n",
    "        total_train_loss = 0\n",
    "        train_batches = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} (Train)\")\n",
    "        \n",
    "        for features, importance_scores, lengths, _ in train_batches:\n",
    "            features, importance_scores = features.to(device), importance_scores.to(device)\n",
    "\n",
    "            # Zero the gradients (clear previous gradients)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: Get predictions from the model\n",
    "            predicted_scores = model(features, lengths)\n",
    "            \n",
    "            # Mask the loss for padded values: Only compute loss on actual data, not padding\n",
    "            # This ensures that padding (zeros) at the end of shorter sequences doesn't affect loss\n",
    "            mask = torch.arange(predicted_scores.size(1)).unsqueeze(0).to(device) < lengths.unsqueeze(1).to(device)\n",
    "            \n",
    "            # Apply mask to both predictions and ground truth scores\n",
    "            masked_predicted_scores = predicted_scores * mask.float()\n",
    "            masked_importance_scores = importance_scores * mask.float()\n",
    "\n",
    "            # Calculate loss based on masked scores\n",
    "            loss = criterion(masked_predicted_scores, masked_importance_scores)\n",
    "\n",
    "            # Backward pass: Compute gradients\n",
    "            loss.backward()\n",
    "            # Optimizer step: Update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss (multiply by batch size for correct average)\n",
    "            total_train_loss += loss.item() * features.size(0) \n",
    "\n",
    "            train_batches.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataset)\n",
    "        print(f\"Epoch {epoch+1} Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # 4. Validation Phase (Evaluate on unseen data after each epoch)\n",
    "        model.eval() # Set model to evaluation mode (turns off dropout, batch norm updates etc.)\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad(): # No gradient computation needed for validation (saves memory/time)\n",
    "            val_batches = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} (Validation)\")\n",
    "            for features, importance_scores, lengths, _ in val_batches:\n",
    "                features, importance_scores = features.to(device), importance_scores.to(device)\n",
    "                \n",
    "                predicted_scores = model(features, lengths)\n",
    "                \n",
    "                mask = torch.arange(predicted_scores.size(1)).unsqueeze(0).to(device) < lengths.unsqueeze(1).to(device)\n",
    "                masked_predicted_scores = predicted_scores * mask.float()\n",
    "                masked_importance_scores = importance_scores * mask.float()\n",
    "                \n",
    "                loss = criterion(masked_predicted_scores, masked_importance_scores)\n",
    "                total_val_loss += loss.item() * features.size(0)\n",
    "                val_batches.set_postfix(loss=loss.item())\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_dataset)\n",
    "        print(f\"Epoch {epoch+1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save model checkpoint if validation loss improves\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            model_save_path = os.path.join(MODEL_SAVE_DIR, f\"summarizer_model_best_val_loss.pth\")\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Model saved to {model_save_path} (New best validation loss: {best_val_loss:.4f})\")\n",
    "        \n",
    "        # You can also save models periodically (e.g., every 10 epochs)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            model_save_path_periodic = os.path.join(MODEL_SAVE_DIR, f\"summarizer_model_epoch_{epoch+1}.pth\")\n",
    "            torch.save(model.state_dict(), model_save_path_periodic)\n",
    "            print(f\"Model saved to {model_save_path_periodic}\")\n",
    "\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be24c351-b534-417c-97a9-8848e08e1544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (inenv)",
   "language": "python",
   "name": "inenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
